# House Price Prediction on Metaflow

This tutorial provides a hands-on introduction to Metaflow using step-by-step instructions for a sample Machine Learning project. The objective of this sample project is to predict the selling price of a house using various attributes like number of bedrooms, number of bathrooms etc. This example uses a Gradient Boosted Regression Tree(GBRT) model to predict the house price.

## Step 1: Setting up your environment

Make sure you have Metaflow installed properly. You can install metaflow by installing the pythonic metaflow library followed by metaflow R package as follows:

[todo: fix the github installation path]
```sh
pip3 install metaflow 
Rscript -e "devtools::install_github('Netflix/metaflow', ref='R')"
```
Installing Metaflow R package from CRAN coming soon!


## Step 2: Creating flow for model training

This example creates a flow called `HousingFlow` as seen in house-price-prediction-flow.R [todo: add link].

Metaflow provides complete flexibility in how users structure their code. Users modularize various functionality needed for data collection, processing, feature extraction, model training, evaluation and model selection as different functions. Metaflow infers the execution graph based on function decorators and functions that link stages in the flow.

- The starting point for the Metaflow execution graph is the step named `start`
- The start step can link to other steps with the `next_step` argument
- The function that is part of a Metaflow step is assigned to `r_function` variable
- The ending point for the Metaflow execution graph is the step named `end`

The execution graph of HousingFlow contains the following steps:

-  **start**  : The starting point of the execution graph is named as `start`. This step is linked to the data collection steps by using `next_step = "load_training_data"`
-  **load_training_data** , **clean_data_set** : These steps are used to collect the input data required to train the model and to prepare and clean it for model training. Please note that these steps can be named as per users’ preference. The only requirement is to have these linked to the execution graph by chaining them at the end of another step.
-  **parameter_grid** : This step is used to enumerate the values for parameters of interest. This method uses the 'foreach' functionality of Metaflow to evaluate the parameter values in parallel across multiple containers in the next step. This is achieved by the line of code that takes advantage of Metaflow’s parallel execution functionality, “foreach” using `next_step = "fit_models", foreach = "parameters"`
-  **fit_models** : This step is used to train a GBRT model for each of the parameter generated by the **parameter_grid** step. This step is launched in parallel across multiple containers.
-  **join** : This step assembles all the multiple model coeffecients from the **fit_model** step.
-  **select_best_fit** : This step performs selects the best model from all candidates using a low value of RMSE as selection criteria.
-  **score_data** : This step performs a batch prediction on an input set of data using the best fit model.
-  **end** : This step is used to indicate the last point in the execution graph.

 HousingFlow can be executed using

 ```R
Rscript  house-price-prediction-flow.R
 ```

 - The option `package_suffixes` in `run` method is used to specify the file suffixes that Metaflow should package as part of the checkpointing process. In this example, a data filed with .csv extension is used to input the training data and using this option tells Metaflow to include the .csv file as part of the package. The default suffices are .py and .R.

Executing the flow produces output similar to the following

FlowExecution [todo:add link]

As seen in the output above, Metaflow assigns a unique `run id`, to each execution of a FlowSpec. Take note of your `run id`, as we will refer to it later. In the example output above `118/start/644565 (pid 12920)`  denotes a `run id` of `118`, and the `task id` of `644565` being associated with the step `start`.

Before every run the code for a step is checkpointed in Metaflow's S3 data repository. After successful completion of a step, the data artifacts of the step are saved in the S3 data repository. These artifacts can be accessed using Metaflow Client API as discussed in the next section for analysis and debugging.


## Step 3: Performing model analysis in Notebook

Once a model is saved as a data artifact in Metaflow data repository, it can be retrieved using Metaflow's Client API for running ad-hoc analysis. Check out the client_notebook.ipynb [todo: fix link] notebook for the next part of the tutorial.